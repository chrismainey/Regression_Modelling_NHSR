---
title: "Regression Modelling in R"
author: "Chris Mainey chris.mainey@uhb.nhs.uk"
date: "2019/09/02 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: "libs/HED.css"
    lib_dir: libs
    seal: false
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---
class: center


```{r setup, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5.2, fig.width=9}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	collapse = TRUE,
	comment = "#>",
	fig.align="center",
	fig.height=5,
	fig.width=9,
	fig.retina=2,
	dev.args = list(type = "cairo")
)
library(Cairo)
library(extrafont)
library(xaringanExtra)
```



<br><br><br><br>

# Regression Modelling in `R`

<br>

.pull-left[
__Chris Mainey__
<p style="font-size:26;")>Senior Data Scientist<br>
<b>University Hospitals Birmingham<br>
NHS Foundation Trust</b></p>
<br>
<span style="font-size:26;">[chris.mainey@uhb.nhs.uk](mailto:chris.mainey@uhb.nhs.uk)</span><br>
`r icon::fa("twitter")` <a href="https://twitter.com/chrismainey?s=09" style="line-height:2;">@chrismainey</a>
]

.pull-right[
<img src= "./man/figures/NHSR-logo.png" width=80% height=80%>
<br><br>
<img src= "./man/figures/HI.png" width=80% height=80%>
]


---

# Workshop Overview

- Correlation
- Linear Regression
 - Specifying a model with `lm`
 - Interpreting the model output
 - Assessing model fit
- Multiple Regression
- Prediction
- Generalized Linear Models using `glm`
 - Logistic Regression
 


<br>
___Mixture of theory, examples and practical exercises___

---

# Relationships between variables

If two variables are related, we usually describe them as 'correlated'.



<br>
Usually interested in "strength" and "direction" of association

--

<br><br>
Two analysis techniques commonly used to investigate:


+ ___Correlation:___ shows direction, and strength of association

+ ___Regression:___ estimate how one variable: $y$ changes when we observed changes in another: $x$ (or more than one, $x_{i}$)

--

<br>
Sometimes the effects of other variables interact/mask this (___"confounding"___)

---

```{r lmsetup, echo=FALSE,include=FALSE}
set.seed(222)
x <- rnorm(50, 10, 4)
y <- runif(50, min = 0.5, 10) + (1.25*x)
z <- predict(lm(y~x))
set.seed(222)
library(ggplot2)
```

## Example:
```{r lm1, fig.retina= 2, fig.align="center"}
x <- rnorm(50, 10, 4)
y <- runif(50, min = 0.5, 10) + (1.25*x)

a<-ggplot(data.frame(x,y,z), aes(x=x,y=y))+
  geom_point(col="dodgerblue2", alpha=0.65)
a
```

---

# Correlation

+ Measured with a correlation coefficient ('Pearson' is the most common)

+ Range: 
 + __-1 to 1:__ Perfect negative to Perfect positive Correlation
 + __0:__  No Correlation

--

<br>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg" height="280" class="center">


.footnote[
Graphic from:
Wikipedia: [Correlation and dependence:](https://en.wikipedia.org/wiki/Correlation_and_dependence)
By DenisBoigelot, https://commons.wikimedia.org/w/index.php?curid=15165296 [Accessed 24 Sept 2019]
]
---

# Correlation in R

Lets check the correlation in our generated data:

```{r correlation}
cor(x, y)

cor.test(x,y)

```

+ `cor.test` is a correlation and a t-test.
+ Different types of correlation coefficient, default is 'Pearson'
+ Doesn't work for different distributions, data types or more variables

---

### Regression models (1)

Regression gives us more options than correlation:

```{r lm3, fig.align="center", fig.retina=2, message=FALSE, warning=FALSE}
z <- lm(y~x)
print(a<- a + geom_smooth(method="lm", col="red", linetype=2))
```


$$y= \alpha + \beta x + \epsilon$$

---

## Regression equation

$$\large{y= \alpha + \beta_{i} x_{i} + \epsilon}$$
<br>
.pull-left[
+ $y$ - is our 'outcome', or 'dependent' variable
+ $\alpha$ - is the 'intercept', the point where our line crosses y-axis
+ $\beta$ - is a coefficient (weight) applied to $x$ 
+ $x$ - is our 'predictor', or 'independent' variable
+ $i$ - is our index, we can have $i$ predictor variables, each with a coefficient
+ $\epsilon$ - is the remaining ('residual') error
]

--

.pull-right[

We are making some assumptions:
+ Linear relations
+ Data points are independent (not correlated)
+ Normally distributed error
+ Homoskedastic (error doesn't vary across the range)
]

---

### Ordinary Least Squares 'OLS'

+ 'Residual' distance between prediction and data point ( $\epsilon$ ).
--

+ Sum would be zero, so we square and minimise _'sum of the squares'_
--

```{r lm2, fig.align="center", message=FALSE, warning=FALSE, fig.height=5.5}
a + geom_segment(aes(xend=c(x), yend=c(z)), size=0.5, arrow=arrow(length=unit(0.2,"cm")))
```

---

## Regression models (3)

+ We created an `lm` object called `z`.  You can then use that object with other functions like `summary`, `predict`, `plot` etc.


```{r sum_reg}
summary(z)
```


--

+ We can test fit using f-tests, prediction error or the R<sup>2</sup> .
+ R<sup>2</sup> is the proportion of variation in $y$, explained by $x$ .


---

## Interpretation 

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

We can now read this as:

+ ___"For each increase of 1 in $x$,  $y$ increases by 1.25, starting at 5.48."___

A common addition is to __"mean-centre and scale"__ our variables:

.panelset[

.panel[.panel-name[Original]
```{r modor}
z <- lm(y~x)
summary(z)
```
]

.panel[.panel-name[Scaled]
```{r modsc}
z <- lm(y~scale(x))
summary(z)
```

]

.panel[.panel-name[Interpretation]
<br><br>
This converts our interpretation:
+ The intercept becomes the average $y$ value
+ $\beta$ becomes the change in $y$ for one standard deviation increase in $x$ 

<br>

So...
+ Average $x$ is 18.24.69
+ Each increase of 1 standard deviation in $x$ increase $y$ by 4.7256
]
]

---

## Regression diagnostics (5)

A common check is to plot residuals:
```{r rplotsetup, eval=FALSE}
plot(z)
```

```{r lmplot, echo=FALSE,  fig.keep = 'last', fig.height=6}
par(mfrow=c(2,2))
plot(z)
```


---
class: center, middle

## Exercise 1:  Linear regression with a single predictor

---

# More than one predictor?

Our plots in earlier slides make sense in 2 dimensions, but regression is not limited to this.

--

<br><br>
If we add more predictos, our interpretation of each coefficient becomes: <br>
+ ___"The change in $y$ whilst holding all others parameters constant"___

<br><br>

We can add more predictors with the `+`:
```{r formula, eval=FALSE}
lm(y ~ x1 + x2 + x3 + xi)
```

---

## Categorical variables

How do we enter categorical variables into a model?

+ Models won't understand text, and numbers are numeric, so we use `factor` variables?

--

<br>
`Factors` are 'dummy coded:' 
+ _'pivotted' to binary columns_
+ Contain a reference level: with categories: "A", "B" & "C", we get:
```{r dummy, echo}
a<- data.frame(Category = factor(c("A", "B", "C", "C", "A", "B")))
model.matrix(~., a)
```

---
class: middle

## Exercise 2:  Linear regression with multiple predictors

---

## What about non-linear data?

- Data are not necessarily linear. Death is binary, LOS is a count etc.

- We can use the Generalized Linear Model (GLM):

$$\large{g(\mu)= \alpha + \beta x}$$
<br>
Where $\mu$ is the expectation of Y, and $g$ is the link function

--

+ The link function transforms the data before fitting a model

--

+ Can't use OLS for this, so we use 'maximum-likelihood'

--

+ Many of the methods for `lm` are common to `glm`, but we can't use R<sup>2</sup>

--

+ Other measures include AUC ('C-statistic'), and AIC or likelihood ratio tests.

---

## Generalized Linear Models

- `family` argument was set to `binomial` for binary outcomes.

```{r glm1, collapse=TRUE, warning=FALSE, message=FALSE}
library(COUNT)
data(medpar)

glm_binomial <- glm(died ~ factor(age80) + los + factor(type), data=medpar, family="binomial")

sjPlot::tab_model(glm_binomial, show.df = TRUE, show.obs = TRUE, show.se = TRUE, show.r2 = FALSE)

ModelMetrics::auc(glm_binomial)
```

---

## Interactions

- 'Interactions' are where predictor variables affect each other.
--

- Can add these with `*` or `:` (check help for which to use)
--

```{r glm2}
glm_binomial2 <- glm(died ~ factor(age80) * los + factor(type), data=medpar, family="binomial")

sjPlot::tab_model(glm_binomial2, show.df = TRUE, show.obs = TRUE, show.se = TRUE, show.r2 = FALSE)
ModelMetrics::auc(glm_binomial2)
```

---

# Interpretation

+ Our model coefficients in `lm` were straight-forward multipliers
+ `glm` is similar, but it is on the scale of the link-function.
 + log scale for `poisson` models, or logit (log odds) scale for `binomial`
+ Common to transform output back to response scale
+ giving Incident Rate Ratios for `poisson`, or Odds Ratios for `binomial`

```{r coeftransform, warning=FALSE, message=FALSE}
cbind(Link=coef(glm_binomial2), Response=exp(coef(glm_binomial2)))
```

---
class: middle

## Exercise 3: Generalized Linear Model (GLM)

---

## Prediction (1)

- We can then use our model to predict our expected $Y$:
- Need to decide what scale to predict on: `link` or `response`

```{r glm3, warning=FALSE, message=FALSE}
library(dplyr)
medpar$preds <- predict(glm_binomial2, type="response")

top_n(medpar,5) %>% knitr::kable(format = "html")
```

---

## Prediction (2)

- Lets see the 10 cases with the highest predicted risk of death:

```{r glm4}
medpar %>% arrange(desc(preds)) %>% top_n(10) %>% knitr::kable(format = "html")
```

---
class: middle

# Exercise 4: Predicting from models

---
# Summary

- Correlation shows the direction and strength of association

- Regression allows us to quantify the relationships

--

- We can use a single, or multiple, predictors 
- Regression coefficients explain how much a change in $x$ affects $y$

--

- R<sup>2</sup> is a common measure of in linear models, C-statistic/AUC/ROC in logistic models

--

- Generalized Linear Model (`glm`) allow linear models on a transformed scale, e.g. logistic regression for binary variables
- Interactions terms allow us to examine confounded predictors

--

- We can predict from our model objects, but must remember the link-function scale in `glm`

---
class: middle

# Exercise 5: Predicting 10-year CHD risk in Framingham data

