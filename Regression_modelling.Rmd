---
title: "Regression Modelling in R"
author: "Chris Mainey chris.mainey@uhb.nhs.uk"
date: "2019/09/02 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    chakra: "libs/remark-latest.min.js"
    css: "libs/HED.css"
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---
class: center


```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  comment = "#>",
  collapse = TRUE,
  fig.align = "center"
)
```



<br><br><br><br>

## Regression Modelling in `R`

<br>

.pull-left[
__Chris Mainey__
<p style="font-size:26;")>Intelligence Analyst<br>
<b>University Hospitals Birmingham 
NHS Foundation Trust</b></p>
<br>
<span style="font-size:26;">[chris.mainey@uhb.nhs.uk](mailto:chris.mainey@uhb.nhs.uk)</span><br>
`r icon::fa("twitter")` <a href="https://twitter.com/chrismainey?s=09" style="line-height:2;">@chrismainey</a>
]

.pull-right[
<img src= "./man/figures/NHSR-logo.png" width=80% height=80%>
<br><br>
<img src= "./man/figures/HI.png" width=80% height=80%>
]


---

# Workshop Overview

- Correlation
- Linear Regression
 - Specifying a model with `lm`
 - Interpreting the model output
 - Assessing model fit
- Multiple Regression
- Prediction
- Generalized Linear Models using `glm`
 - Logistic Regression
 
--

<br>
___Mixture of theory, examples and practical exercises___

---

# Relationships between variables

If two variables are related, we usually describe them as 'correlated'.

--

<br>
Often interested in strength of association

--

<br><br>
Two analysis techniques commonly used to investigate:

--

+ ___Correlation:___ shows direction, and strength of association

--

+ ___Regression:___ estimates how one variable, $x$ (or more than one), predicts another, $y$ .

--

<br><br>
Sometimes the effects of other variables interact/mask this ("confounding")

---

```{r lmsetup, echo=FALSE,include=FALSE}
set.seed(222)
x <- rnorm(50, 10, 4)
y <- runif(50, min = 0.5, 10) + (1.25*x)
z <- predict(lm(y~x))
set.seed(222)
library(ggplot2)
```

## Example:
```{r lm1, fig.retina= 2, fig.align="center", fig.height=4.6, fig.width=6}
x <- rnorm(50, 10, 4)
y <- runif(50, min = 0.5, 10) + (1.25*x)

a<-ggplot(data.frame(x,y,z), aes(x=x,y=y))+
  geom_point(col="dodgerblue2", alpha=0.65)
a
```

---

# Correlation

+ Measured with a correlation coefficient ('Pearson' is the most common)

+ Range: 
 + __-1 to 1:__ Perfect negative to Perfect positive Correlation
 + __0:__  No Correlation
--

<br><br>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg" height= 60% align="center">


.footnote[
Graphic from:
Wikipedia: [Correlation and dependence:](https://en.wikipedia.org/wiki/Correlation_and_dependence)
By DenisBoigelot, https://commons.wikimedia.org/w/index.php?curid=15165296 [Accessed 24 Sept 2019]
]
---

# Correlation in R

Lets check the correlation in our generated data:

```{r correlation}
cor(x, y)

cor.test(x,y)

```

+ `cor.test` is a correlation and a t-test.
+ Different types of correlation coefficient, default is 'Pearson'
+ Doesn't work for different distributions, data types or more variables

---

### Regression models (1)

Regression gives us more options than correlation:

```{r lm3, fig.retina=2, fig.align="center", fig.height=4.8, fig.width=6}
z <- lm(y~x)

print(a<- a + geom_smooth(method="lm", col="red", linetype=2))
```


$$y= \alpha + \beta x + \epsilon$$

---

## Regression equation

$$y= \alpha + \beta x + \epsilon$$
+ $y$ - is our 'outcome variable'
+ $\alpha$ - is the 'intercept', the point where our line crosses y-axis
+ $\beta$ - is a coefficient (weight) applied to $x$ 
+ $x$ - is our predictor variable
+ $\epsilon$ - is the remaining ('residual') error

--
<br><br>

We assume:
+ Linear relations
+ Data points are independent (not correlated)
+ Normally distributed error
+ Homoskedastic (error doesn't vary much)


---

### Ordinary Least Squares 'OLS'

+ 'Residual' distance between prediction and data point ($\epsilon$).
--

+ Sum would be zero, square and minimise 'sum of the squares'
--

```{r lm2, fig.retina=2, fig.align="center", fig.height=5, fig.width=6}
a + geom_segment(aes(xend=c(x), yend=c(z)), size=0.5, arrow=arrow(length=unit(0.2,"cm")))
```

---

## Regression models (3)

+ We created an `lm` object called `z`.  You can then use that object with other functions like `summary`, `predict`, `plot` etc.


```{r sum_reg}
summary(z)
```


---

## Regression models (4)

+ Usually view model details with `summary`, but I'm using a function from `sjPlot` here.
```{r lm4}
sjPlot::tab_model(z, show.df = TRUE, show.obs = TRUE)

```

--

<br>
+ We can test fit using f-tests, prediction error or the R<sup>2</sup> .
+ R<sup>2</sup> is the proportion of variation in $y$, explained by $x$ .


---

## Interpretation 

We can now read this as:

_For each increase of 1 in $x$,  $y$ increases by 1.25, starting at 5.48._

<br><br>

A common addition is to mean-centre and scale our variables:
+ Take away the mean, and divide by standard deviation (z-score).

--

<br><br>
This converts our interpretation, so:
+ The intercept becomes the average $y$ value
+ $\beta$ becomse the change in $y$ for each standard deviation increase in $x$ 


---

## Regression diagnostics (5)

- A common check is to plot residuals:
```{r rplotsetup, echo=FALSE}
par(mfrow=c(2,2))
```

```{r lmplot, fig.retina=2, fig.align="center", fig.height=5.5, fig.width=8, fig.keep = 'last'}
plot(z)
```


---
class: center, middle

## Exercise 1:  Linear regression with single predictor

---

# More than one predictor?

Our plots in earlier slide make sense in 2 dimensions, but regression is not limited by this.

--

<br><br>
Our interpretation of each coefficient becomes: <br>
___The change in $y$ whilst holding all others constant___

<br><br>

We can add more predictors with the `+`:
```{r formula, eval=FALSE}
lm(y ~ x1 + x2 + x3 + xn)
```

---
class: center, middle

## Exercise 2:  Linear regression with multiple predictors

---

## What about non-linear data?

- Our data are not necessarily linear. Death is binary, LOS is a count etc.

- Can extend linear models ideas to the Generalized Linear Model (GLM)

$$\large{g(\mu)= \alpha + \beta x}$$
<br>
Where $\mu$ is the expectation of Y, and $g$ is the link function

--

+ The link function transforms the data before fitting a model

--

+ Can't use OLS for this, so we use 'maximum-likelihood'

--

+ Many of the methods for `lm` are common to `glm`, but we can't use R<sup>2</sup>

--

+ Other measures include AUC ('C-statistic'), and AIC or likelihood ratio tests.

---

## Generalized Linear Models

- Use a `family` argument with `poisson` for counts or `binomial` for binary outcomes.

```{r glm1, collapse=TRUE, warning=FALSE, message=FALSE}
library(COUNT)
data(medpar)

glm_binomial <- glm(died ~ factor(age80) + los + factor(type), data=medpar, family="binomial")

sjPlot::tab_model(glm_binomial, show.df = TRUE, show.obs = TRUE, show.se = TRUE, show.r2 = FALSE)
ModelMetrics::auc(glm_binomial)
```

---

## Interactions

- 'Interactions' are where predictor variables affect each other.
--

- HSMR has an interaction between co-morbidity and age. So co-morbidities have different effects related to age.
--

- Can add these with `*` or `:` (check help for which to use)
--

```{r glm2}
glm_binomial2 <- glm(died ~ factor(age80) * los + factor(type), data=medpar, family="binomial")

sjPlot::tab_model(glm_binomial2, show.df = TRUE, show.obs = TRUE, show.se = TRUE, show.r2 = FALSE)

ModelMetrics::auc(glm_binomial2)

```

---

# Interpretation

+ Our model coefficients in `lm` were straight-forward multipliers
+ `glm` is similar, but it is on the scale of the link-function.
 + log scale for `poisson` models, or logit (log odds) scale for `binomial`
+ Common to transform output back to response scale
+ giving Incident Rate Ratios for `poisson`, or Odds Ratios for `binomial`

```{r coeftransform, warning=FALSE, message=FALSE}
cbind(Link=coef(glm_binomial2), Response=exp(coef(glm_binomial2)))
```

---
class: middle, center

# Exercise 3: Generalized Linear Model (GLM)

---

## Prediction (1)

- We can then use our model to predict our expected Y:
- Need to decide what scale to predict on: `link` or `response`

```{r glm3, warning=FALSE, message=FALSE}
library(dplyr)
medpar$preds <- predict(glm_binomial2, type="response")

top_n(medpar,5) %>% knitr::kable(format = "html")
```

---

## Prediction (2)

- Lets see the 10 cases with the highest predicted risk of death:

```{r glm4}
medpar %>% arrange(desc(preds)) %>% top_n(10) %>% knitr::kable(format = "html")

```

---
class: middle

# Exercise 2: Building a GLM

---
# Summary

- Correlation shows the direction and strength of association
- Regression allows us to quantify the relationships
- We can use a single, or multiple, predictors 
- Regression coefficents explain how much a change in $x$ affects $y$
- R<sup>2</sup> is a common measure of in linear models, C-statistic/AUC/ROC in logistic moels
- Prediction error is another useful metric
- Generalized Linear Model (`glm`) allow linear models on a tranfomred scale, e.g. logositc regression for binary variables

---
class: middle

# Exercise 5: Predicting 10-yeard CHD in Frammingham data

